---
title: "Web_Scraping_Project"
author: "Luca Cincera"
date: "2 Juni 2020"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

# Important notes

The scraping tiles are deactivated by default (eval = FALSE) thus it should be blocked to scrape the webpages every time. The websites were scraped once and the two data files were exported and could be loaded as needed. 

# Import Packages

Import all the necessary libraries for the project and create the connection to github.

```{r, echo = FALSE, warning = FALSE}
library("knitr")

library("rvest")

library("stringr")
library("dplyr")
library("ggplot2")

library("lubridate")
```


```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
opts_knit$set(root.dir = "~/GitHub_Projects/Web_Scraping_Project")
opts_chunk$set(fig.align = "center")
```

# Web-Scraping Part

Following two Web-Scrapers were created. The first Web-Scraper collects data on alarms from the Winterthur fire brigade and the second Web-Scraper collects certain weather data for the Winterthur region.

## Application reports Web-Scraper



```{r, eval = FALSE}
df_alarms <- data.frame(date = as.Date(character()), hour = integer(), duration = double(), alarm_type = character(), location = character(), firefighters = integer())

for (j in 2017:2020){
  
  url <- paste("https://einsatzberichte-feuerwehr.winterthur.ch/mission-reports?page=0&year=", j , sep="")
  webpage <- read_html(url)
  anz <- webpage %>% html_nodes("b") %>% html_text() %>% as.integer()
  anz <- as.integer(ceiling(anz/10)-1)
  
  for (i in 0:anz){
  
    url <- paste("https://einsatzberichte-feuerwehr.winterthur.ch/mission-reports?page=", i, "&year=", j , sep="")
  
    webpage <- read_html(url)
    
    d <- webpage %>% html_nodes("tr td") %>% html_text() %>% as.matrix()
    
    date <- d[seq(from = 1, to = 60, by = 6)] %>% str_replace_all(pattern = "\n", replacement = "") %>% str_trim("both")
    time <- d[seq(from = 2, to = 60, by = 6)] %>% str_replace_all(pattern = "\n", replacement = "") %>% str_trim("both")
    duration <- d[seq(from = 3, to = 60, by = 6)] %>% str_replace_all(pattern = "\n", replacement = "") %>% str_replace_all(pattern = "h", replacement = "") %>% str_trim("both")
    alarm_type <- d[seq(from = 4, to = 60, by = 6)] %>% str_replace_all(pattern = "\n", replacement = "") %>% str_trim("both") %>% str_replace_all(pattern = ":.*", replacement = "")
    location <- d[seq(from = 5, to = 60, by = 6)] %>% str_replace_all(pattern = "\n", replacement = "") %>% str_trim("both")
    firefighters <- d[seq(from = 6, to = 60, by = 6)] %>% str_replace_all(pattern = "\n", replacement = "") %>% str_trim("both") %>% str_sub(start = 1, end = 1)
    
    df_t <- data.frame(date = as.Date(date,"%d.%m.%Y"), hour = as.integer(substr(time,1,2)), duration = as.double(duration), 
                       alarm_type, location, firefighters = as.integer(firefighters), stringsAsFactors = FALSE)
    
    df_alarms <- bind_rows(df_alarms, df_t)
    
    Sys.sleep(3)
    print(paste("Is running j=", j ," and i=", i, sep = ""))
  }
}

df_alarms <- df_alarms %>% filter(!is.na(date)) # remove NA-values which were collected by mistake
df_alarms$alarm_type <- as.factor(df_alarms$alarm_type)
df_alarms$location <- as.factor(df_alarms$location)
```

## Weather Web-Scraper

```{r, eval = FALSE}
df_weather <- data.frame(date = as.Date(character()), temp = double(), rain = double(), wind = double(), suntime = double(), storm = double())

monate <- c("01","02","03","04","05","06","07","08","09","10","11","12")

for (j in 2017:2020){
  
  for (i in 1:12){
    
    m <- monate[i]
    
    url <- paste("http://www.winti-wetter.ch/", j,"/m", j, m,".htm",sep="")
  
    webpage <- read_html(url)
    
    data <- webpage %>% html_nodes("tr td") %>% html_text() %>% as.matrix()
    
    date <- data[seq(from = 2, to = 23*31, by = 23)]
    temp <- data[seq(from = 3, to = 23*31, by = 23)] %>% str_replace_all(pattern = "°C", replacement = "") %>% str_trim("both")
    rain <- data[seq(from = 12, to = 23*31, by = 23)] %>% str_replace_all(pattern = "l/m²", replacement = "") %>% str_trim("both")
    wind <- data[seq(from = 14, to = 23*31, by = 23)] %>% str_replace_all(pattern = "km/h", replacement = "") %>% str_trim("both")
    suntime <- data[seq(from = 17, to = 23*31, by = 23)] %>% str_replace_all(pattern = "h", replacement = "") %>% str_trim("both")
    storm <- data[seq(from = 23, to = 23*31, by = 23)] %>% str_replace_all(pattern = "km/h", replacement = "") %>% str_trim("both")
    
    df_t <- data.frame(date = as.Date(date,"%d.%m.%Y"), temp = as.double(temp), rain = as.double(rain), wind = as.double(wind), suntime = as.double(suntime), storm = as.double(storm))
  
    df_weather <- bind_rows(df_weather, df_t)
    
    Sys.sleep(3)
    print(paste("Is running j=", j ," and i=", i, sep = ""))
    
    if (j == 2020 && i == 5){break} # break after Mai 2020
  }
}

df_weather <- df_weather %>% filter(!is.na(date)) # remove NA-values due months with less than 31 days
```

## Save the DataFrames

After the Web-Scraping both DataFrames will be saved. The simple reason is that with this behaviour the data has to be scraped only once. 

```{r, eval = FALSE}
write.csv(df_alarms, file = "df_alarms.csv")
write.csv(df_weather, file = "df_weather.csv")
```

# Analysis

```{r}
df_alarms <- read.csv(file = "df_alarms.csv", header = TRUE)
df_weather <- read.csv(file = "df_weather.csv", header = TRUE)

df_alarms$date <- as.Date(df_alarms$date)
df_weather$date <- as.Date(df_weather$date)
```

## Data Exploration

```{r}
ggplot(df_alarms, aes(x=hour)) + geom_histogram(binwidth = 1, color="darkblue", fill="lightblue") + 
  labs(x = "hour", y = "Alarm count", title = "Histogram of Alarms per Hour")

df_sum_alarms <- df_alarms %>% group_by(alarm_type) %>% summarise(count = n()) %>% top_n(10) 
ggplot(data=df_sum_alarms, aes(x=reorder(alarm_type, count), y=count)) + geom_bar(stat="identity", color="darkblue", fill="lightblue") + coord_flip() +
  labs(x = "Alarm count", y = "Type of Alarm", title = "Top 10 Alarm Types")



```


## Combine Heat with Fire

```{r}
alarms_by_month <- df_alarms %>% filter(str_detect(alarm_type, "Brand")) %>% group_by(month = months(date)) %>% 
  summarise(count = n()/(3*12+5)) # To get the aveage

temp_by_month <- df_weather %>% group_by(month = months(date)) %>% filter(year(date) == "2019") %>% summarise(avg_temp = mean(temp))

df_comb <- alarms_by_month %>% left_join(temp_by_month, by="month")
df_comb$season <- as.factor(c(1,2,4,4,4,2,2,1,1,3,3,3))


ggplot(df_comb, aes(x=count, y=avg_temp, label=month, colour=season)) + geom_text() + 
  scale_color_manual(values=c("darkorange","red","darkgreen","blue")) +
  labs(x = "Average Fires per day", y = "Average temperatur", title = "Scatterplot Average Fires vs. Average Temperature")
```




